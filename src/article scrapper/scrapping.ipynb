{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Overview\n",
    "\n",
    "In this notebook, we will discuss our journey in web scraping. Although we developed numerous scrapers tailored for different websites, our experience with scraping about half of them led us to create a generalized scraper using Selenium. This versatile scraper enables us to efficiently gather data from various sites, streamlining our scraping process and enhancing productivity.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep, time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.select import Select\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import urllib.parse\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "from json import dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(('/').join(os.getcwd().split('/')[:-2]), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver(website):\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)  # Use custom download directory\n",
    "    options.set_preference(\"browser.download.dir\", os.path.join(data_dir, website))\n",
    "    options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"text/csv\") \n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, source):\n",
    "    if source == 'en':\n",
    "        return text\n",
    "    try:\n",
    "        return GoogleTranslator(source=source, target='en').translate(text=text).replace(',', '').replace('\\n', '')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def create_directories(base_url, categories):\n",
    "    # create the following dir struct; outputs > base website > categories\n",
    "    base_dir = os.path.join(data_dir, urlparse(base_url).netloc)\n",
    "    base_links_dir = os.path.join('../scrapping/links', urlparse(base_url).netloc)\n",
    "\n",
    "    if not os.path.exists(base_links_dir):\n",
    "        os.makedirs(base_links_dir)\n",
    "\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "\n",
    "    for category, _ in categories:\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        if not os.path.exists(category_dir):\n",
    "            os.makedirs(category_dir)\n",
    "        with open(os.path.join(base_links_dir), f'{category}.txt', 'w') as f:\n",
    "            f.write('')\n",
    "\n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to capture image data when a user clicks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_urls(website, category):\n",
    "    with open(f'../scrapping/links/{website}/{category}.txt', 'r') as f:\n",
    "        return f.read().split('\\n')\n",
    "\n",
    "def get_java_script():\n",
    "    with open('scripts/script.js', 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def article_scrapper(website, category, driver):\n",
    "    article_links = load_urls(website, category)\n",
    "    js = get_java_script()\n",
    "\n",
    "    for x, link in enumerate(article_links):\n",
    "        driver.get(link)\n",
    "        driver.execute_script(js)\n",
    "        input(f'You may begin selecting images for article {x+1}')\n",
    "\n",
    "    driver.execute_script(f'{js.replace(\"file_name\", category)}\\n\\nendSelection()')\n",
    "\n",
    "def format_df(file_path):\n",
    "    df = pd.read_csv(file_path, quotechar='\"')\n",
    "    df.drop_duplicates(subset=['article_heading', 'img_src'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    article_mapping = {url: idx + 1 for idx, url in enumerate(df['article_url'].unique())}\n",
    "    df['article_number'] = df['article_url'].map(article_mapping)\n",
    "    df['image_number'] = df.groupby('article_number').cumcount() + 1  \n",
    "\n",
    "    return df[['article_number' 'image number', 'img_src', 'altText', 'article_heading', 'article_url']].rename(columns={'altText': 'alt', 'image_number' : 'image number'} )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(img_url, save_dir, img_name):\n",
    "    try:\n",
    "        if not img_url.startswith('data:'):\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(img_url, headers=headers)\n",
    "            img_data = response.content\n",
    "            img = Image.open(BytesIO(img_data))\n",
    "            width, height = img.size\n",
    "\n",
    "            # Only save images larger than 100x100 pixels\n",
    "            if width >= 100 and height >= 100:\n",
    "                with open(os.path.join(save_dir, img_name), 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def download_images(df, save_dir):\n",
    "\n",
    "    # parallising the downloads to make it faster\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        headlines = []\n",
    "        for idx in range(df.shape[0]):\n",
    "            img_url = df.loc[idx, 'img_src']\n",
    "            img_name = df.loc[idx, 'image number']\n",
    "            futures.append(executor.submit(download_image, img_url, save_dir, img_name))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of the Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ('Health', 'https://muzyka.interia.pl/pop,nPack,2'),\n",
    "    ('Tech', 'https://tech.163.com/')\n",
    "    ('Military', 'https://war.163.com/')\n",
    "    ('International Football', 'https://sports.163.com/world')\n",
    "    ('World News', 'https://news.163.com/world')\n",
    "]\n",
    "\n",
    "base_url = 'https://www.163.com/'\n",
    "base_dir = create_directories(base_url, categories)\n",
    "base_lang = 'cn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Scraper**\n",
    "\n",
    "After running the cell above, directories will be created in `links/{website}/{category}.txt`. Each of these text files will contain 10 article links relevant to the specified category. This structure allows for organized access to add lastest 10 article links for each category. Add these links in appropiate files and then run the next cell to scrape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, _ in tqdm(categories, desc='Downloading images for every category'):\n",
    "    website = urlparse(base_url).netloc\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "\n",
    "    driver = initialize_driver(website)\n",
    "    article_scrapper(website, category, driver)\n",
    "\n",
    "    sleep(5)\n",
    "\n",
    "    df = format_df(f'{data_dir}/{website}/{category}.csv')\n",
    "    download_images(df, category_dir)\n",
    "\n",
    "    df.drop(columns=['img_src'], inplace=True)\n",
    "    df['article_heading'] = df['article_heading'].apply(translate, src=base_lang)\n",
    "    df['alt'] = df['alt'].apply(translate, src=base_lang)\n",
    "    df.to_csv(f'{category_dir}/image_data.csv', index=False)\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes temporary files made to mantain images source during scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, _ in categories:\n",
    "    os.remove(os.path.join(base_dir, f'{category}.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
